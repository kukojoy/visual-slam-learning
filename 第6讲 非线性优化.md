# 第6讲 非线性优化

## 6.1 状态估计问题

### 6.1.1 批量状态估计和最大后验估计

经典的SLAM模型由运动方程和观测方程构成：
$$
\left \{ \begin{matrix}
\boldsymbol{x}_k = f(\boldsymbol{x}_{k - 1}, \boldsymbol{u}_k) + \boldsymbol{w}_k \\
\boldsymbol{z}_{k,j} = h(\boldsymbol{x}_k, \boldsymbol{y}_j) + \boldsymbol{v}_{k,j}
\end{matrix} \right.
$$
其中，$\boldsymbol{x}_k$表示$k$时刻的相机位姿，$\boldsymbol{y}_j$表示路标点$j$的三维世界坐标。

对于**观测方程**，假设不考虑噪声$\boldsymbol{v}_{k,j}$的话，有：
$$
s\boldsymbol{z}_{k, j} = \boldsymbol{K}(\boldsymbol{R}_k\boldsymbol{y}_j + \boldsymbol{t}_k)
$$
其中，$\boldsymbol{z}_{k,j}$为$k$时刻路标点$j$投影的像素坐标，$s$为路标点$j$在$k$时刻相机坐标系下坐标的第三个分量。

现在考虑数据受到噪声的影响，假设$\boldsymbol{w}_k, \boldsymbol{v}_{k,j}$服从零均值的多维高斯分布：
$$
\boldsymbol{w}_k \sim N(\boldsymbol{0}, \boldsymbol{R}_k), \boldsymbol{v}_{k,j} \sim N(\boldsymbol{0}, \boldsymbol{Q}_{k, j})
$$
此时，我们要通过带噪声的数据$\boldsymbol{z}, \boldsymbol{u}$估计位姿$\boldsymbol{x}$和地图$\boldsymbol{y}$，这构成了一个状态估计问题。

处理这个状态估计问题的方法大致分为两种，**增量/渐进**法（或者叫滤波器）和**批量**法。

增量法：首先，根据**状态方程**（即运动方程），利用上一时刻的状态（即位姿）$\boldsymbol{x}_{k-1}$和当前运动传感器的数据$\boldsymbol{u}_k$，估计出当前时刻的状态$\boldsymbol{x}_{k}(estimation)$，然后用当前时刻观测到的数据$\boldsymbol{z}_k$反过来对$\boldsymbol{x}_{k}(estimation)$进行更新。具体来说，是通过观测误差（即实际观测到的数据$\boldsymbol{z}_k$，与将$\boldsymbol{x}_{k}(estimation)$代入观测方程得到的结果$\boldsymbol{z}_k(estimatiom)$的差距）来修正$\boldsymbol{x}_{k}(estimation)$，得到当前状态$\boldsymbol{x}_{k}$。

批量法：思想是将数据都攒起来一并处理。例如，将$0\sim k$时刻所有的输入和观测数据都放在一起，并利用这些攒起来的数据估计整个$0 \sim k$时刻的轨迹和地图。

总的来说，增量方法仅关心当前时刻状态$\boldsymbol{x}_k$的求解，而批量方法是对$0 \sim k$时刻的所有状态进行求解，可以在更大的范围达到最优化，被认为优于传统的滤波器，是当前视觉SLAM的主流方法。极端情况下，可以先收集全部的数据，再带回计算中心统一处理，但这种极端情况是**不实时**的，不符合SLAM的应用场景。因此，在SLAM中会使用一些折衷的方法，例如，在$k$时刻，就对$k-9 \sim k$时刻的所有状态进行优化（而非$0 \sim k$的全部时刻或者只考虑$k$时刻），称为**滑动窗口估计**法。

首先，学习以非线性优化为主的批量优化方法。考虑从$1 \sim N$的所有时刻，假设有$M$个路标点。定义所有时刻机器人的位姿和路标点坐标为：
$$
\boldsymbol{x} = \{\boldsymbol{x}_1, \dots, \boldsymbol{x}_N\}, \quad \boldsymbol{y} = \{\boldsymbol{y}_1, \dots, \boldsymbol{y}_M\}
$$
同样的，用$\boldsymbol{u}$代表所有时刻的输入，即运动传感器数据。用$\boldsymbol{z}$代表所有时刻的观测数据。那么，现在要解决的问题，从概率学的角度看，就是已知$\boldsymbol{u}, \boldsymbol{z}$，求$\boldsymbol{x}, \boldsymbol{y}$的条件概率分布，即：
$$
P(\boldsymbol{x}, \boldsymbol{y} | \boldsymbol{u}, \boldsymbol{z})
$$
有时候，我们并不知道$\boldsymbol{u}$，仅有观测数据（一张张的图像），相当于只考虑观测方程来估计条件概率分布$P(\boldsymbol{x}, \boldsymbol{y} | \boldsymbol{z})$，此问题称为SfM。

利用贝叶斯法则，有：
$$
P(\boldsymbol{x}, \boldsymbol{y} | \boldsymbol{u}, \boldsymbol{z}) = \frac{P(\boldsymbol{u}, \boldsymbol{z}| \boldsymbol{x}, \boldsymbol{y} )P(\boldsymbol{x}, \boldsymbol{y})}{P(\boldsymbol{u}, \boldsymbol{z})}\propto
P(\boldsymbol{u}, \boldsymbol{z}| \boldsymbol{x}, \boldsymbol{y})P(\boldsymbol{x}, \boldsymbol{y})
$$
其中，$P(\boldsymbol{x}, \boldsymbol{y} | \boldsymbol{u}, \boldsymbol{z})$称为后验，$P(\boldsymbol{u}, \boldsymbol{z}| \boldsymbol{x}, \boldsymbol{y})$称为似然，$P(\boldsymbol{x}, \boldsymbol{y})$称为先验。直接求后验分布是很困难的，但是求一个最优状态估计，使得在该状态下有最大后验概率（MAP）是可行的。即可以求解：
$$
(\boldsymbol{x}, \boldsymbol{y})^* = \arg \max P(\boldsymbol{x}, \boldsymbol{y} | \boldsymbol{u}, \boldsymbol{z}) = \arg \max P(\boldsymbol{u}, \boldsymbol{z}| \boldsymbol{x}, \boldsymbol{y})P(\boldsymbol{x}, \boldsymbol{y})
$$
贝叶斯展开式的分母部分与$\boldsymbol{x}, \boldsymbol{y}$无关，因而可以省略，故求解最大后验对应的状态等价于求解最大似然和先验乘积对应的状态。很多情况下，我们也不知道先验，那么，可以求解最大似然估计（MLE）：
$$
(\boldsymbol{x}, \boldsymbol{y})^* =  \arg \max P(\boldsymbol{u}, \boldsymbol{z}| \boldsymbol{x}, \boldsymbol{y})
$$
直观理解，就是在求解”在什么样的状态$(\boldsymbol{x}, \boldsymbol{y})$下，最匹配输入$\boldsymbol{u}$，最可能观测到数据$\boldsymbol{z}$“。

### 6.1.2 最小二乘的引出

对于某一次观测，有：
$$
\boldsymbol{z}_{k, j} = h(\boldsymbol{x}_k, \boldsymbol{y}_j) + \boldsymbol{v}_{k, j}
$$
其中噪声项$\boldsymbol{v}_{k,j} \sim N(\boldsymbol{0}, \boldsymbol{Q}_{k, j})$，所以：
$$
P(\boldsymbol{z}_{k, j}|\boldsymbol{x}_{k},\boldsymbol{y}_j) = N(h(\boldsymbol{x}_k, \boldsymbol{y}_j), \boldsymbol{Q}_{k, j})
$$
这里用分布$N$来指代对应的概率密度函数，即在$\boldsymbol{x}_k, \boldsymbol{y}_j$条件下，观测数据$\boldsymbol{z}_{k, j}$服从多元高斯分布。考虑**单次观测**的最大似然估计，可以用最小化负对数来求。

首先，给出一般的多元高斯分布并对其取负对数：
$$
P(\boldsymbol{x}) = \frac{1}{\sqrt{(2 \pi)^N\det(\boldsymbol{\Sigma})}}\exp \left ( - \frac{1}{2}(\boldsymbol{x} - \boldsymbol{u})^\top \boldsymbol{\Sigma}^{-1}(\boldsymbol{x} - \boldsymbol{u})  \right )
$$

$$
- \ln(P(\boldsymbol{x})) = \frac{1}{2} \ln \left( (2\pi)^N \det(\boldsymbol{\Sigma})\right) + \frac{1}{2}\left(\boldsymbol{x} - \boldsymbol{u})^\top \boldsymbol{\Sigma}^{-1}(\boldsymbol{x} - \boldsymbol{u})  \right )
$$

对应到上述单次观测的情况，可以分析：

$\boldsymbol{x}$对应了给定$(\boldsymbol{x}_k, \boldsymbol{y}_j)$时的观测$\boldsymbol{z}_{k, j}$，$\boldsymbol{\Sigma}$对应了协方差矩阵$\boldsymbol{Q}_{k, j}$，这些是已知的，因此上式中的$\ln \left( (2\pi)^N \det(\boldsymbol{\Sigma})\right)$与状态无关。要优化的东西是$\boldsymbol{u}$，即$h(\boldsymbol{x}_k, \boldsymbol{y}_j)$，也就是$(\boldsymbol{x}_k, \boldsymbol{y}_j)$。所以：
$$
(\boldsymbol{x}_k, \boldsymbol{y}_j)^* = \arg \max \left(N(h(\boldsymbol{x}_k, \boldsymbol{y}_j), \boldsymbol{Q}_{k, j}) \right) = \arg \min \left((\boldsymbol{z}_{k, j} - h(\boldsymbol{x}_k, \boldsymbol{y}_j))^\top \boldsymbol{Q}_{k, j}^{-1}(\boldsymbol{z}_{k, j} - h(\boldsymbol{x}_k, \boldsymbol{y}_j))\right)
$$
该式等价于最小化观测误差（即$\boldsymbol{z}_{k, j} - h(\boldsymbol{x}_k, \boldsymbol{y}_j)$）的一个二次型。这个二次型称为马哈拉诺比斯距离，也叫马氏距离。它也可以看成由$Q_{k, j}^{-1}$加权之后的欧式距离（二范数），这里$Q_{k, j}^{-1}$也成为信息矩阵，即高斯分布协方差矩阵的逆。

现在，考虑**批量时刻**的数据。假设各个时刻的输入（注：在上面单次观测分析中，为了简单，没有考虑输入）和观测是相互独立的，则：
$$
P(\boldsymbol{u}, \boldsymbol{z}| \boldsymbol{x}, \boldsymbol{y} ) = \prod_k P(\boldsymbol{u}_k|\boldsymbol{x}_{k - 1}, \boldsymbol{x}_k) \prod_{k,j} P(\boldsymbol{z}_{k, j}|\boldsymbol{x}_k, \boldsymbol{y}_j)
$$
定义各次输入和观测数据与模型之间的误差：
$$
\boldsymbol{e}_{\boldsymbol{u}, k} = \boldsymbol{x}_k - f(\boldsymbol{x}_{k - 1}, \boldsymbol{u}_k) \\
\boldsymbol{e}_{\boldsymbol{z}, k, j} = \boldsymbol{z}_{k, j} - h(\boldsymbol{x}_{k}, \boldsymbol{y}_j) \\
$$
那么，求批量时刻的最大似然估计，等价于最小化批量时刻的估计值与真实值之间的马氏距离：
$$
\min J(\boldsymbol{x}, \boldsymbol{y}) = \sum_k \boldsymbol{e}_{\boldsymbol{u}, k}^\top \boldsymbol{R}^{-1}_k\boldsymbol{e}_{\boldsymbol{u}, k} + \sum_k\sum_j\boldsymbol{e}_{\boldsymbol{z}, k, j}^\top \boldsymbol{Q}_{k, j}^{-1}\boldsymbol{e}_{\boldsymbol{z}, k, j}
$$
这样就得到了一个最小二乘问题，它的解是状态的最大似然估计。直观理解，因为噪声误差的存在，运动方程和观测方程都不会完美成立，真实值与观测值之间会存在误差。此时，就要对状态进行微调，使得整体误差下降，到达一个极小值，这就是一个典型的非线性优化过程。

现在，要探讨的问题是如何”对状态进行微调“，求解非线性优化问题，这在SLAM前端，后端都有应用。

### 6.1.3 例子：批量状态估计

考虑一个非常简单的线性离散时间系统：
$$
\begin{matrix}
\begin{array}{l}
\boldsymbol{x}_k = \boldsymbol{x}_{k - 1} + \boldsymbol{u}_k + \boldsymbol{w}_k, & \boldsymbol{w}_k \sim N(\boldsymbol{0}, \boldsymbol{Q}_k) \\
\boldsymbol{z}_k = \boldsymbol{x}_k + \boldsymbol{n}_k, & \boldsymbol{n}_k \sim N(\boldsymbol{0}, \boldsymbol{R}_k)
\end{array}
\end{matrix}
$$
假设有一辆在平面中行驶的汽车。以第一个公式作为运动方程，$\boldsymbol{u}_k$为输入。第二个公式作为状态方程，$\boldsymbol{z}_k$为对汽车位置的测量。$\boldsymbol{w}_k, \boldsymbol{n}_k$为噪声。设初始状态$\boldsymbol{x}_0$已知，取时间$k = 1, 2, 3$，批量状态变量$\boldsymbol{x} = [\boldsymbol{x}_0, \boldsymbol{x}_1, \boldsymbol{x}_2, \boldsymbol{x}_3]^\top$，批量输入$\boldsymbol{u} = [\boldsymbol{u}_1, \boldsymbol{u}_2, \boldsymbol{u}_3]^\top$，批量观测$\boldsymbol{z} = [\boldsymbol{z}_1, \boldsymbol{z}_2, \boldsymbol{z}_3]^\top$。根据这些数据，可以求最大似然估计：
$$
\boldsymbol{x}^* = \arg \max(P(\boldsymbol{x}|\boldsymbol{u}, \boldsymbol{z})) = \arg \max(P(\boldsymbol{u}, \boldsymbol{z}|\boldsymbol{x})) \\
= \prod_{k = 1}^3 P(\boldsymbol{u}_k|\boldsymbol{x}_{k - 1}, \boldsymbol{x}_k) \prod_{k = 1}^3 P(\boldsymbol{z}_k|\boldsymbol{x}_k)
$$
其中，
$$
P(\boldsymbol{u}_k|\boldsymbol{x}_{k - 1}, \boldsymbol{x}_k) = N(\boldsymbol{x}_{k - 1} + \boldsymbol{u}_{k }, \boldsymbol{Q}_k) \\
P(\boldsymbol{z}_k|\boldsymbol{x}_k) = N(\boldsymbol{x}_k, \boldsymbol{R}_k)
$$
构建误差变量：
$$
\boldsymbol{e}_{\boldsymbol{u}, k} = \boldsymbol{x}_k - \boldsymbol{x}_{k - 1} - \boldsymbol{u}_k \\
\boldsymbol{e}_{\boldsymbol{z}, k} = \boldsymbol{z}_{k} - \boldsymbol{x}_{k} \\
$$
得到最小二乘的目标函数：
$$
\min \sum_{k = 1}^3 \boldsymbol{e}_{\boldsymbol{u}, k}^\top \boldsymbol{Q}^{-1}_k\boldsymbol{e}_{\boldsymbol{u}, k} + \sum_{k = 1}^3 \boldsymbol{e}_{\boldsymbol{z}, k}^\top \boldsymbol{R}^{-1}_{k}\boldsymbol{e}_{\boldsymbol{z}, k}
$$
由于例中运动方程和观测方程都是线性的，这个最小二乘目标函数中的求和可以写成矩阵形式：
$$
\boldsymbol{e} = \boldsymbol{y} - \boldsymbol{Hx} \sim N(\boldsymbol{0}, \boldsymbol{\Sigma})
$$
其中，$\boldsymbol{y} = [\boldsymbol{u}, \boldsymbol{z}]^\top = [\boldsymbol{u}_1,\boldsymbol{u}_2,\boldsymbol{u}_3,\boldsymbol{z}_1,\boldsymbol{z}_2,\boldsymbol{z}_3]^\top$，$\boldsymbol{\Sigma} = \mathrm{diag}(\boldsymbol{Q}_1, \boldsymbol{Q}_2,\boldsymbol{Q}_3, \boldsymbol{R}_1, \boldsymbol{R}_2, \boldsymbol{R}_3)$。

注：$\boldsymbol{\Sigma}$写成这个形式的前提是$\boldsymbol{u}, \boldsymbol{z}$独立。
$$
\boldsymbol{H} = \begin{bmatrix} 
-1 & 1 & 0 & 0 \\ 
0  & -1 & 1 & 0 \\
0 & 0 & -1 & 1 \\
0 & 1 & 0 & 0 \\
0 & 0 & 1 & 0 \\
0 & 0 & 0 & 1 \\
\end{bmatrix}
$$
最终可以得到解析解：
$$
\boldsymbol{x}^* = \arg \min \boldsymbol{e}^\top\boldsymbol{\Sigma}^{-1}\boldsymbol{e} = (\boldsymbol{H}^\top\boldsymbol{\Sigma}^{-1}\boldsymbol{H})^{-1}\boldsymbol{H}^\top\boldsymbol{\Sigma}^{-1}\boldsymbol{y}
$$

## 6.2 非线性最小二乘

考虑一个简单的最小二乘问题的目标函数：
$$
\min_{\boldsymbol{x}} F(\boldsymbol{x}) = \frac{1}{2} \|f(\boldsymbol{x})\|_2^2
$$
其中，自变量$\boldsymbol{x} \in \mathbb{R}^n$，$f$是任意标量非线性函数$f(\boldsymbol{x}):\mathbb{R}^n \to \mathbb{R}$。如果$f$是个数学形式上很简单的函数，那么可以用解析形式求解，令梯度为$\boldsymbol{0}$，求解$\boldsymbol{x}$的最优值：
$$
\frac{dF}{d\boldsymbol{x}} = \boldsymbol{0}
$$
解这个方程，代入$F$，就得到了极大、极小或鞍点处的值，只需要逐个比较就能确定最小值，并得到相应的$\boldsymbol{x}^*$。但是，如果$f$的形式过于复杂，通常求不出解析解，则需要用迭代的方式，从一个初始值出发，不断更新待优化的变量，使目标函数下降，具体步骤如下：

1. 给定某个初始值$\boldsymbol{x}_0$
2. 对于第$k$次迭代，寻找一个增量$\Delta \boldsymbol{x}_k$，使得$\|f(\boldsymbol{x}_k + \Delta \boldsymbol{x}_k)\|_2^2$尽可能小于$\|f(\boldsymbol{x}_k)\|_2^2$
3. 若$\Delta \boldsymbol{x}_k$足够小，即目标函数收敛，则停止
4. 否则，令$\boldsymbol{x}_{k+1} = \boldsymbol{x}_k + \Delta \boldsymbol{x}_k$，返回第2步

这样一来，问题就变成了寻找能够使目标函数下降的增量$\Delta \boldsymbol{x}_k$，由于$\Delta \boldsymbol{x}_k$本身是一个不大的值，因此可以在$\boldsymbol{x}_k$附近的一个局部范围内把$f$线性化，简化增量$\Delta \boldsymbol{x}_k$的计算。

对于$\Delta \boldsymbol{x}_k$的计算，有许多被广泛使用的经典方法，如一阶梯度法（最速梯度下降法）、二阶梯度法（牛顿法）、高斯牛顿法和列文伯格-马夸尔特法。

### 6.2.1 一阶和二阶梯度法

考虑第$k$次迭代，假设在$\boldsymbol{x}_k$处，想要寻到增量$\Delta \boldsymbol{x}_k$，最直观的方式是将目标函数在$\boldsymbol{x}_k$附近进行泰勒展开：
$$
F(\boldsymbol{x}_k + \Delta \boldsymbol{x}_k) \approx F(\boldsymbol{x}_k) + \boldsymbol{J}^\top(\boldsymbol{x}_k) \Delta \boldsymbol{x}_k + \frac{1}{2}\Delta \boldsymbol{x}_k^\top\boldsymbol{H}(\boldsymbol{x}_k)\Delta \boldsymbol{x}_k
$$
如果保留一阶项或二阶项，那么对应的方法被称为一阶或二阶梯度法。

**一阶梯度法（最速梯度下降法）**是取增量为负梯度与一个步长的乘积，即：
$$
\Delta \boldsymbol{x}_k^* = -\lambda \boldsymbol{J}(\boldsymbol{x}_k)
$$
其中步长$\lambda$可以根据一定条件来计算，在机器学习中（被称为学习率）也可以用一些经验性的方法来确定。

**为什么要乘这个步长：**保证目标函数局部的一阶（线性）近似，当步长足够小时，迭代后的函数值必然下降。当然，步长太小会造成迭代次数增加，速度太慢，因此要折衷。

**二阶梯度法（牛顿法）**是将目标函数在$\boldsymbol{x}_k$附近的局部看作是$\Delta \boldsymbol{x}_k$的函数，并求解这个函数的极小值，即：
$$
\Delta \boldsymbol{x}_k^* = \arg \min \left( F(\boldsymbol{x}_k) + \boldsymbol{J}^\top(\boldsymbol{x}_k) \Delta \boldsymbol{x}_k + \frac{1}{2}\Delta \boldsymbol{x}_k^\top\boldsymbol{H}(\boldsymbol{x}_k)\Delta \boldsymbol{x}_k \right)
$$
具体可以令函数对增量的导数为零来求解，即：
$$
\boldsymbol{H}(\boldsymbol{x}_k)\Delta \boldsymbol{x}_k = -\boldsymbol{J}(\boldsymbol{x}_k)
$$
求解这个线性方程，便得到了增量。

**总结：**一阶和二阶梯度法都很直观，将目标函数在迭代点附近进行泰勒展开，求解增量并进行迭代，只要目标函数的局部能够近似为一次或二次函数（通常情况下都是可行的），算法就能正常工作。然而，这两种方法都有自身的缺点，一阶法过于贪心，容易走出锯齿路线，反而增加迭代次数。二阶法需要计算$\boldsymbol{H}$，当问题规模较大时非常麻烦，这通常是需要避免的。对于非线性最小二乘问题，**高斯牛顿法**和**列文伯格-马夸尔特法**则更加实用。

### 6.2.2 高斯牛顿法

高斯牛顿法的思想是将$f(\boldsymbol{x})$（而不是$F(\boldsymbol{x})$）进行一阶泰勒展开：
$$
f(\boldsymbol{x} + \Delta \boldsymbol{x}) \approx f(\boldsymbol{x}) + \boldsymbol{J}^\top(\boldsymbol{x})\Delta \boldsymbol{x}
$$
当前的目标仍然是找到增量$\boldsymbol{x}$，使得$\|f(\boldsymbol{x} + \Delta \boldsymbol{x})\|_2^2$尽量小，即：
$$
\Delta \boldsymbol{x}^* = \arg \min \frac{1}{2} \|f(\boldsymbol{x}) + \boldsymbol{J}^\top(\boldsymbol{x})\Delta \boldsymbol{x}\|_2^2
$$
将目标函数展开：
$$
\frac{1}{2} \|f(\boldsymbol{x}) + \boldsymbol{J}^\top(\boldsymbol{x})\Delta \boldsymbol{x}\|_2^2

= \frac{1}{2}\left(f(\boldsymbol{x}) + \boldsymbol{J}^\top(\boldsymbol{x})\Delta \boldsymbol{x}\right)^\top \left(f(\boldsymbol{x}) + \boldsymbol{J}^\top(\boldsymbol{x})\Delta \boldsymbol{x}\right) \\

= \frac{1}{2}\left(\|f(\boldsymbol{x})\|_2^2 + f^\top(\boldsymbol{x})\boldsymbol{J}^\top(\boldsymbol{x}) \Delta \boldsymbol{x} + \Delta \boldsymbol{x}^\top \boldsymbol{J}(\boldsymbol{x})f(\boldsymbol{x}) + \Delta \boldsymbol{x}^\top\boldsymbol{J}(\boldsymbol{x})\boldsymbol{J}^\top(\boldsymbol{x})  \Delta \boldsymbol{x}\right)
$$
根据极值条件，对$\Delta \boldsymbol{x}$求导并让导数为零：
$$
\boldsymbol{J}(\boldsymbol{x})f(\boldsymbol{x}) + \boldsymbol{J}(\boldsymbol{x})\boldsymbol{J}^\top(\boldsymbol{x})\Delta \boldsymbol{x} = 0
$$
将这个关于$\Delta \boldsymbol{x}$的方程组记为：
$$
\boldsymbol{H}(\boldsymbol{x}) \Delta \boldsymbol{x} = \boldsymbol{g}(\boldsymbol{x})
$$
称为增量方程/高斯牛顿方程/正规方程。求解增量方程是整个优化问题的核心，其步骤可以归纳如下：

1. 给定初始值$\boldsymbol{x}_0$
2. 对于第$k$次迭代，求出当前的误差$f(\boldsymbol{x}_k)$和雅可比矩阵$\boldsymbol{J}(\boldsymbol{x}_k)$

3. 求解增量方程$\boldsymbol{H}(\boldsymbol{x}_k) \Delta \boldsymbol{x}_k = \boldsymbol{g}(\boldsymbol{x}_k)$
4. 若$\Delta \boldsymbol{x}_k$足够小，即目标函数收敛，则停止迭代。否则，令$\boldsymbol{x}_{k+1} = \boldsymbol{x}_k + \Delta \boldsymbol{x}_k$，返回第2步

至此，我们知道，只要能顺利解出增量，就能保证目标函数能够正确地下降。但是，这里有几个需要解释的点：

1. 增量方程中$\boldsymbol{H}(\boldsymbol{x})$的记法是有意义的，高斯牛顿法用$\boldsymbol{J}(\boldsymbol{x})\boldsymbol{J}^\top(\boldsymbol{x})$作为海塞矩阵的近似，省略了计算过程，弥补了牛顿法的不足。
2. $\boldsymbol{H}(\boldsymbol{x})$实际可能为奇异矩阵（不可逆）或病态矩阵（可逆但数值计算不稳定，得到的增量容易有大的误差），导致算法不收敛。
3. 计算出来的增量$\Delta\boldsymbol{x}$可能太大，导致采用的局部近似值不够准确，无法保证迭代收敛，甚至可能使目标函数变大。

在非线性优化领域，很多算法可归结为高斯牛顿法的变种，借助其思想并通过自己的改进修正其缺点。例如引入一个步长$\alpha$，用式$\boldsymbol{x}_{k+1} = \boldsymbol{x}_k + \alpha \Delta \boldsymbol{x}_k$对增量进行更新而不是简单地让$\alpha = 1$，避免了超出局部近似范围的问题。

列文伯格—马夸尔特方法在一定程度上修正了这些问题，一般认为它比高斯牛顿法的鲁棒性更强，但收敛速度可能会更慢，被称为阻尼牛顿法。

### 6.2.3 列文伯格—马夸尔特方法

在高斯牛顿法中，二阶泰勒展开式只在展开点附近有较好的近似效果，因此可以给$\Delta \boldsymbol{x}$设定一个二阶近似的有效范围，称为**信赖区域**。

一个比较好的确定信赖区域范围的方法是根据近似模型和实际函数之间的差异，可以定义一个指标$\rho$来衡量这个差异：
$$
\rho = \frac{f(\boldsymbol{x} + \Delta \boldsymbol{x}) - f(\boldsymbol{x})}{\boldsymbol{J}^\top(\boldsymbol{x})\Delta \boldsymbol{x}}
$$
如果$\rho$接近1，则近似是好的。如果$\rho$太小，说明实际下降的比预测下降的要小，近似较差，需要缩小近似范围。如果$\rho$太大，说明实际下降的比预测下降的要多，可以放大近似范围。下面给出具体的算法框架：

1. 给定初始值$\boldsymbol{x}_0$，和初始优化半径$\mu_0$

2. 对于第$k$次迭代，求解：
   $$
   \min_{\Delta \boldsymbol{x}_k} \frac{1}{2}\|f(\boldsymbol{x}_k + \boldsymbol{J}^\top(\boldsymbol{x}_k)\Delta \boldsymbol{x}_k)\|_2^2, \quad s.t \quad\|\boldsymbol{D}\Delta \boldsymbol{x}_k\|^2_2 \leq \mu_k
   $$
   其中，$\mu_k$为当前信赖区域的半径，$\boldsymbol{D}$为系数矩阵，稍后对其进行说明。

3. 计算$\rho$，若$\rho > \frac{3}{4}$，则令$\mu_{k + 1} = 2\mu_k$。若$\rho < \frac{1}{4}$，则令$\mu_{k + 1} = 0.5\mu_k$

4. 如果$\rho$大于某个阈值，则认为近似是好的。令$\boldsymbol{x}_{k + 1} = \boldsymbol{x}_{k} + \Delta \boldsymbol{x}_k$

5. 判断算法是否收敛，如收敛，则结束，否则返回第2步

实际上，算法第2步的约束条件的含义就是将$\boldsymbol{D}\Delta \boldsymbol{x}_k$限制在一个半径为$\mu_k$的球（这个球可能是多维的）中，即将$\Delta \boldsymbol{x}_k$限制在一个椭球中。在列文伯格提出的优化方法中，将$\boldsymbol{D}$取为单位阵$\boldsymbol{I}$，相当于直接将$\Delta \boldsymbol{x}_k$约束在一个求中。马夸尔特则提出令$\boldsymbol{D}$为非负对角阵（实际中通常用$\boldsymbol{J}^\top \boldsymbol{J}$）的对角元素平方根，使得在梯度小的维度上约束范围更大一些。

无论如何，这个优化问题的核心都是求解第2步中$\Delta \boldsymbol{x}_k$，可以用拉格朗日乘数法，构造拉格朗日函数来求最值：
$$
L(\Delta \boldsymbol{x}_k, \lambda) = \frac{1}{2}\|f(\boldsymbol{x}_k + \boldsymbol{J}^\top(\boldsymbol{x}_k)\Delta \boldsymbol{x}_k)\|_2^2 + \frac{\lambda}{2}(\|\boldsymbol{D}\Delta \boldsymbol{x}_k\|^2_2 - \mu_k)
$$
对$\Delta \boldsymbol{x}_k$求导并让导数为零：
$$
(\boldsymbol{H}(\boldsymbol{x}_k) + \lambda \boldsymbol{D}^\top \boldsymbol{D}) \Delta \boldsymbol{x}_k = \boldsymbol{g}(\boldsymbol{x}_k)
$$
**注意：**这里从数学上分析，应该还要考虑对$\lambda$求导并让导数为零得到一个式子，然后联立上面这个式子来求解。但在这里是将$\lambda$看作一个已知的权重，称为惩罚项，并在每一次迭代中，根据计算结果对惩罚项进行合理的调整，只要最后算法能顺利工作即可。

考虑$\boldsymbol{D} = \boldsymbol{I}$的简单情况，当$\lambda$比较小时，$\boldsymbol{H}$占主要地位，说明二阶近似的效果比较好，列文伯格—马夸尔特法更接近高斯牛顿法。当二阶近似的效果不够好，即$\lambda$比较大时，会更接近于一阶梯度下降法。这说明了列文伯格—马夸尔特方法的稳定性较好。

实际中还有很多求解增量的方法。但在视觉SLAM中，高斯牛顿法和列文伯格—马夸尔特法是最基本也是最常用的方法。如果问题的性质不好（即$\boldsymbol{H}(\boldsymbol{x})$为奇异矩阵或病态矩阵），用列文伯格—马夸尔特方法。如果问题性质较好，则用高斯牛顿。

**思考：**

1. 在做优化计算时，提供良好的初始值是非常重要的，并且提供初始值的方法要有理论依据，而不是直接暴力随机
2. 应该如何快速求解线性增量方程组

## 6.3 实践：曲线拟合问题

### 6.3.1 手写高斯牛顿法

### 6.3.2 使用ceres库

### 6.3.3 使用g2o库

具体见代码。手写高斯牛顿对掌握编程和强化对理论的理解是非常有帮助的。
