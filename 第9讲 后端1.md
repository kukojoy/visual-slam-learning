# 第9讲 后端1

## 9.1 概述

### 9.1.1 状态估计的概率解释

回顾之前所学的知识，假设在$t = 0 \sim N $时间内，有位姿$\boldsymbol{x}_0, \dots, \boldsymbol{x}_N$，并且有路标$\boldsymbol{y}_1, \dots, \boldsymbol{y}_M$，那么，SLAM过程可以由运动方程和观测方程来描述：
$$
\begin{array}{l}
\left \{ \begin{matrix}
\boldsymbol{x}_k = f(\boldsymbol{x}_{k - 1}, \boldsymbol{u}_k) + \boldsymbol{w}_k \\
\boldsymbol{z}_{k, j} = h(\boldsymbol{y}_j, \boldsymbol{x}_k) + \boldsymbol{v}_{k, j}
\end{matrix} \right.
\end{array}
\quad k = 1, \dots, N, j = 1, \dots, M
$$
**两个要注意的点：**

1. 观测方程中，在某个时刻$k$通常只能看到一小部分路标$\boldsymbol{y}_j$，产生与看到的路标相应的观测方程，如果一个路标都没看到，那就没有观测方程。在视觉SLAM中，特征点数量众多，实际中观测方程数量会远远大于运动方程的数量。
2. 我们可能没有测量运动的装置（即$\boldsymbol{u}_k = 0$），甚至没有运动方程。在没有运动方程时，有三种处理方法：一，认为确实没有运动方程。二，假设相机不动。三，假设相机匀速运动。在没有运动方程的情况下，整个优化问题只由许多个观测方程组成，即仅通过一组图像来做SLAM。

我们知道，每个方程都会受到噪声的影响，所以要把位姿$\boldsymbol{x}$和路标$\boldsymbol{y}$都看作服从某种概率分布的随机变量（通常是高斯分布，在计算中关注其均值和协方差即可。均值是对变量最优值的估计，协方差矩阵则衡量了不确定性）。最后，问题转变为：当已知一组运动数据$\boldsymbol{u}$和观测数据$\boldsymbol{z}$时，如何来估计状态量$\boldsymbol{x}, \boldsymbol{y}$？

举一个直观的例子来定性理解：假设我们蒙着眼在一个未知的地方走路，尽管我们能较好的估计自己每一步大概走了多远（每次估计的误差可以看作是噪声），但是随着时间流逝，我们越来越不确定自己自己的位置，不确定自己从开始到现在究竟走了多远，这就是**误差的积累**。但当我们睁开眼睛时观测到外部场景，对位置估计的不确定性就会变小，能更自信地估计自己的位置。

下面，以定量的方式看待这个问题，在第6讲中，知道了批量状态估计问题可以转化为最大似然估计问题，并用最小二乘法进行求解。在本节将继续探讨如何将该结论应用于**渐进式问题**，得到一些经典的结论。同时，在视觉SLAM中，由于非线性优化方程组中**矩阵的特殊结构**，我们所构建的最小二乘问题有相应的特殊求解方法。

**思考备注：**这里说的渐进式问题可能与滑动窗口法有关，也就是SLAM在批量方法中的折衷。矩阵的特殊结构可能是第6讲提到的稀疏性。

首先，位姿$\boldsymbol{x}$和路标$\boldsymbol{y}$都是待估计的变量，这里为了方便，将它们统称为**状态量**，并改变记号，令$k$时刻的状态：
$$
\boldsymbol{x}_k = \{\boldsymbol{x}_k, \boldsymbol{y}_1, \dots, \boldsymbol{y}_m\}
$$
**注意：**式子花括号中的$\boldsymbol{x}_k$还是位姿的意思，而路标$\boldsymbol{y}$的下标到$m$而不是$M$，因为每个时刻观测到的路标是不一样的，不可能每次都观测到所有的$M$个点。

同时，把$k$时刻的所有观测记作$\boldsymbol{z}_k = \{\boldsymbol{z}_{k, 1}, \dots, \boldsymbol{z}_{k, m}\}$，此时，运动方程和观测方程的形式可写得更简洁：
$$
\left \{ 
\begin{array}{l}
\begin{matrix}
\boldsymbol{x}_k = f(\boldsymbol{x}_{k - 1}, \boldsymbol{u}_k) + \boldsymbol{w}_k \\
\boldsymbol{z}_{k} = h(\boldsymbol{x}_k) + \boldsymbol{v}_{k}
\end{matrix} 
\end{array} \right.
\quad k = 1, \dots, N
$$
现在，考虑时刻$k$的情况，我们希望用$0 \sim k$中的数据来估计现在的状态分布：
$$
P(\boldsymbol{x}_k|\boldsymbol{x}_0, \boldsymbol{u}_{1:k}, \boldsymbol{z}_{1:k})
$$
**思考备注：**直接从运动方程看，$\boldsymbol{x}_k$与$\boldsymbol{x}_{k - 1}$有关，但是上面这个状态分布的条件中并没有$\boldsymbol{x}_{k - 1}$，反而有前$k$个时刻的输入$\boldsymbol{u}$和观测$\boldsymbol{z}$，这应该是一种隐式的表达法，因为各时刻的状态都是由直接获取的输入和观测计算而来的。

利用贝叶斯公式将上式展开，交换$\boldsymbol{x}_k, \boldsymbol{z}_k$有：
$$
P(\boldsymbol{x}_k|\boldsymbol{x}_0, \boldsymbol{u}_{1:k}, \boldsymbol{z}_{1:k}) = \frac{P(\boldsymbol{z}_k|\boldsymbol{x}_k, \boldsymbol{x}_0, \boldsymbol{u}_{1:k}, \boldsymbol{z}_{1:k - 1})P(\boldsymbol{x}_k|\boldsymbol{x}_0, \boldsymbol{u}_{1:k}, \boldsymbol{z}_{1:k - 1})}{P(\boldsymbol{z}_k|\boldsymbol{x}_0, \boldsymbol{u}_{1:k}, \boldsymbol{z}_{1:k - 1})} \propto 
P(\boldsymbol{z}_k|\boldsymbol{x}_k)P(\boldsymbol{x}_k|\boldsymbol{x}_0, \boldsymbol{u}_{1:k}, \boldsymbol{z}_{1:k - 1})
$$
**思考备注：**为什么交换$\boldsymbol{x}_k, \boldsymbol{z}_k$？因为$\boldsymbol{x}_k$是要求的，而$\boldsymbol{z}_k$是当前时刻获取的观测，主要就是通过这两个东西来将后验转化成似然和先验的乘积，再求最大似然估计。为什么等价于最右边的式子（为什么这个正比符号是合理的）？因为条件$\boldsymbol{x}_k$比条件$\boldsymbol{x}_0, \boldsymbol{u}_{1:k}, \boldsymbol{z}_{1:k - 1}$要更强，且根据观测方程来看，确实知道了$\boldsymbol{x}_k$后，$\boldsymbol{z}_k$的分布就确定了，有没有$\boldsymbol{x}_0, \boldsymbol{u}_{1:k}, \boldsymbol{z}_{1:k - 1}$都一样。分母也是定值（$\boldsymbol{z}_k, \boldsymbol{x}_0, \boldsymbol{u}_{1:k}, \boldsymbol{z}_{1:k - 1}$都已知）。

似然部分，可以由观测方程给定。先验部分，$\boldsymbol{x}_k$是基于过去的所有状态估计得来的，可以以先前时刻的状态为条件展开：
$$
P(\boldsymbol{x}_k|\boldsymbol{x}_0, \boldsymbol{u}_{1:k}, \boldsymbol{z}_{1:k - 1}) = \int P(\boldsymbol{x}_k|\boldsymbol{x}_{k - 1}, \boldsymbol{x}_0, \boldsymbol{u}_{1:k}, \boldsymbol{z}_{1:k - 1})P(\boldsymbol{x}_{k - 1}|\boldsymbol{x}_0, \boldsymbol{u}_{1:k}, \boldsymbol{z}_{1:k - 1}) \mathrm{d}\boldsymbol{x}_{k - 1}
$$
如果考虑之前更久的状态，则可以继续对上式进行展开。

至此，给出了贝叶斯估计，得到了似然和先验。对这一步后续的处理，方法上产生了分歧，大体上有两种方法：一是假设**马尔可夫性**，简单的一阶马氏性认为，$k$时刻状态只与$k - 1$时刻有关，基于这个假设，就可以得到以**扩展卡尔曼滤波（EKF）**为代表的滤波器方法，从某个时刻的状态出发，推导下一个时刻的状态。二是考虑$k$时刻状态与之前所有状态的关系，得到**非线性优化**为主体的框架，这是视觉SLAM的主流。

### 9.1.2 线性系统和KF

尽管不是滤波器不是主流方法，也有学习和了解的必要，对开阔视野和学习其他方法都有帮助。所以这里也记录一些。

当假设了马尔可夫性，上面的条件状态展开式的第一部分：
$$
P(\boldsymbol{x}_k|\boldsymbol{x}_{k - 1}, \boldsymbol{x}_0, \boldsymbol{u}_{1:k}, \boldsymbol{z}_{1:k - 1}) = P(\boldsymbol{x}_k|\boldsymbol{x}_{k - 1}, \boldsymbol{u}_{k})
$$
这实际上就是运动方程表现的显示形式。第二部分：
$$
P(\boldsymbol{x}_{k - 1}|\boldsymbol{x}_0, \boldsymbol{u}_{1:k}, \boldsymbol{z}_{1:k - 1}) = P(\boldsymbol{x}_{k - 1}|\boldsymbol{x}_0, \boldsymbol{u}_{1:k - 1}, \boldsymbol{z}_{1:k - 1})
$$
这实际上就是$\boldsymbol{x}_{k - 1}$的状态分布。于是，这一系列方程说明，我们实际在做的是如何把$k - 1$时刻的状态分布推导至$k$时刻这样一件事。也就是说，在优化过程中，只需要维护一个状态量，不断对其进行更新迭代，如果假设其服从高斯分布，那么只需考虑维护状态量的均值和协方差。将均值视为状态的最优值，将协方差视为估计的不确定性。

下面，从形式最简单的线性高斯系统开始，最后得到卡尔曼滤波器：
$$
\left \{
\begin{matrix}
\begin{array}{l}
\boldsymbol{x}_k = \boldsymbol{A}_k\boldsymbol{x}_{k - 1} + \boldsymbol{u}_k + \boldsymbol{w}_k \\
\boldsymbol{z}_k = \boldsymbol{C}_k\boldsymbol{x}_k + \boldsymbol{v}_k
\end{array}
\end{matrix}
\quad k = 1, \dots, N
\right .
$$
其中，$\boldsymbol{w}_k \sim N(\boldsymbol{0}, \boldsymbol{R}_k), \boldsymbol{v}_k \sim N(\boldsymbol{0}, \boldsymbol{Q}_k)$。

利用马尔可夫性，卡尔曼滤波器的计算可以归纳为“预测”和“更新”两个步骤：

1. 预测（利用运动方程计算先验）
   $$
   \check{\boldsymbol{x}}_k = \boldsymbol{A}_k\hat{\boldsymbol{x}}_{k - 1} + \boldsymbol{u}_k \\
   \check{\boldsymbol{P}}_k = \boldsymbol{A}_k\hat{\boldsymbol{P}}_{k - 1}\boldsymbol{A}_k^\top + \boldsymbol{R}_k
   $$

2. 更新（利用观测方程计算后验）

   先计算卡尔曼增益$\boldsymbol{K}_k = \check{\boldsymbol{P}}_k\boldsymbol{C}_k^\top(\boldsymbol{C}_k\check{\boldsymbol{P}}_k\boldsymbol{C}_k^\top + \boldsymbol{Q}_k)^{-1}$

   然后，计算后验：
   $$
   \hat{\boldsymbol{x}}_k = \check{\boldsymbol{x}}_k + \boldsymbol{K}_k(\boldsymbol{z}_k - \boldsymbol{C}_k\check{\boldsymbol{x}}_k) \\
   \hat{\boldsymbol{P}}_k = (\boldsymbol{I} - \boldsymbol{K}_k\boldsymbol{C}_k)\check{\boldsymbol{P}}_k
   $$

其中，符号 $\hat{}$ 表示后验，$\check{}$ 表示先验，$\boldsymbol{P}_k$代表$k$时刻状态量的协方差矩阵。

**备注：**推导可以根据上一小节的结论，从最大后验概率估计出发，转化成求解求似然和先验乘积的最大，具体的推导过程略。

**结论：**在线性高斯系统中，卡尔曼滤波器构成了系统的最大后验概率估计，由于高斯分布经过线性变换后仍服从高斯分布，整个的过程没有任何近似，因此卡尔曼滤波器构成了线性系统的最优无偏估计。

### 9.1.3 非线性系统和EKF

SLAM系统是非线性系统，运动方程和观测方程都是非线性方程，而一个高斯分布经过非线性变换后，往往不再是高斯分布，所以，我们要进行一定的近似，将一个非高斯分布近似成高斯分布，才能将卡尔曼滤波器的结果拓展到非线性系统中，称为扩展卡尔曼滤波器（EKF）。通常的做法是，在某个点附近考虑运动方程及观测方程的一阶泰勒展开，保留线性的部分，然后按线性系统的方法推导。

假设$k - 1$时刻的均值和协方差矩阵为$\hat{\boldsymbol{x}}_{k - 1}, \hat{\boldsymbol{P}}_{k - 1}$。在$k$时刻，把运动方程和观测方程在$\hat{\boldsymbol{x}}_{k - 1}, \hat{\boldsymbol{P}}_{k - 1}$处进行线性化：
$$
\boldsymbol{x}_k = f(\boldsymbol{x}_{k - 1}, \boldsymbol{u}_k) + \boldsymbol{w}_k \approx f({\hat{\boldsymbol{x}}_{k - 1}}, \boldsymbol{u}_k) + \left . \frac{\partial f}{\partial \boldsymbol{x}_{k - 1}}\right |_{\boldsymbol{x}_{k - 1} = \hat{\boldsymbol{x}}_{k - 1}}(\boldsymbol{x}_{k - 1} - \hat{\boldsymbol{x}}_{k - 1}) + \boldsymbol{w}_k \\
\boldsymbol{z}_k \approx h(\check{\boldsymbol{x}}_k) + \left. \frac{\partial h}{\partial \boldsymbol{x}_k} \right|_{\boldsymbol{x}_k = \check{\boldsymbol{x}}_k}(\boldsymbol{x}_{k} - \check{\boldsymbol{x}}_{k}) + \boldsymbol{v}_k
$$
记：
$$
\boldsymbol{F} = \left . \frac{\partial f}{\partial \boldsymbol{x}_{k - 1}} \right |_{\boldsymbol{x}_{k - 1} = \hat{\boldsymbol{x}}_{k - 1}}, \boldsymbol{H} = \left. \frac{\partial h}{\partial \boldsymbol{x}_k} \right|_{\boldsymbol{x}_k = \check{\boldsymbol{x}}_k}
$$
那么，在EKF的**预测**步骤中，有：
$$
\check{\boldsymbol{x}}_k = f({\hat{\boldsymbol{x}}_{k - 1}}, \boldsymbol{u}_k) \\
\check{\boldsymbol{P}}_k = \boldsymbol{F}_k\hat{\boldsymbol{P}}_{k - 1}\boldsymbol{F}_k^\top + \boldsymbol{R}_k
$$
在**更新**步骤中，有：
$$
\boldsymbol{K}_k = \check{\boldsymbol{P}}_k\boldsymbol{H}_k^\top(\boldsymbol{H}_k\check{\boldsymbol{P}}_k\boldsymbol{H}_k^\top + \boldsymbol{Q}_k)^{-1}\\
\hat{\boldsymbol{x}}_k = \check{\boldsymbol{x}}_k + \boldsymbol{K}_k(\boldsymbol{z}_k - h(\check{\boldsymbol{x}}_k)) \\
\hat{\boldsymbol{P}}_k = (\boldsymbol{I} - \boldsymbol{K}_k\boldsymbol{H}_k)\check{\boldsymbol{P}}_k
$$
卡尔曼滤波器给出了在线性化之后状态变量分布的变化过程。在线性系统和高斯噪声下，卡尔曼滤波器给出了无偏最优估计。在SLAM非线性系统中，也可以给出单次线性近似下的最大后验估计。

### 9.1.4 EKF的讨论

EKF的形式简洁，应用广泛，在早期的SLAM中占据主导地位。但是，其存在以下局限性：

1. 滤波器方法假设了**马尔可夫性**，认为$k$时刻状态只与$k - 1$时刻的状态有关，类似在视觉里程计中只考虑相邻两帧的关系。如果当前帧确实与很久之前的数据有关（例如回环），那么滤波器会难以处理。
2. EKF的线性化有时会出现近似不可靠的情况，会带来**非线性误差**。
3. EKF需要存储状态量的均值和协方差，不适合大型场景状态量非常多，存储空间需求大的情况。
4. 没有异常检测机制。在视觉SLAM中，异常值是很常见的，且异常值会导致计算发散，因此实用中非常不稳定。

由于EKF存在上述缺点，因此我们通常认为，在计算量相当的情况下，非线性优化的精度和鲁棒性都要高于滤波器方法。但是，在资源受限或待估计量比较简单的场合，EKF也不失为一种有效的方式。

接下来，讨论以非线性优化为主的后端。

## 9.2 BA与图优化

BA（Bundle Adjustment）是指从视觉图像中提炼出最优的3D模型（地图）和相机参数（内参数和外参数）。考虑从任意特征点发射出来的几束光线（bundles of light rays），它们会在几个相机的成像平面上变成像素或特征点。如果调整（adjustment）各相机姿态和各特征点的空间位置，使这些光线最终收束到相机的光心，就称为BA。

本节的重点：介绍BA对应的图模型结构的特点及其通用的快速求解方法。

### 9.2.1 投影模型和BA代价函数

回顾整个投影的过程：



这个过程就是观测方程：
$$
\boldsymbol{z} = h(\boldsymbol{x}, \boldsymbol{y})
$$
这里的$\boldsymbol{x}$就是位姿，即外参数$\boldsymbol{R}, \boldsymbol{t}$，对应的李群为$\boldsymbol{T}$，李代数为$\boldsymbol{\xi}$。$\boldsymbol{y}$就是三维点$\boldsymbol{p}$，观测数据$\boldsymbol{z}$为像素坐标$[u_s, v_s]^\top$。

以最小二乘的角度来考虑，此次观测的误差为：
$$
\boldsymbol{e} = \boldsymbol{e}(\boldsymbol{T}, \boldsymbol{p}) = \boldsymbol{z} - h(\boldsymbol{T}, \boldsymbol{p})
$$
设$\boldsymbol{z}_{k, j}$为$k$时刻位姿$\boldsymbol{T}_k$处观察路标$\boldsymbol{p}_j$产生的观测数据，则考虑所有时刻观测的整体误差（代价函数）为：
$$
\frac{1}{2} \|\boldsymbol{e}_\mathrm{all}\|^2_2 = \frac{1}{2} \sum^N_{k = 1} \sum^M_{j = 1}\|\boldsymbol{e}_{k, j}\|^2_2 = \frac{1}{2} \sum^N_{k = 1} \sum^M_{j = 1}\|\boldsymbol{e}(\boldsymbol{T}_k, \boldsymbol{p}_j)\|^2_2  = \frac{1}{2} \sum^N_{k = 1} \sum^M_{j = 1}\|\boldsymbol{z}_{k, j} - h(\boldsymbol{T}_k, \boldsymbol{p}_j)\|^2_2
$$
对这个最小二乘进行求解，相当于对位姿和路标同时做调整，也就是BA。接下来，讨论该模型的求解。

###  9.2.2 BA的求解

根据非线性优化的思想，我们应该从某个初始值开始，不断求解增量得到$\Delta \boldsymbol{x}$来找到目标函数的最优解。在整体BA目标函数上，应该把自变量定义成所有待优化的变量：
$$
\boldsymbol{x} = [\boldsymbol{T}_1, \dots, \boldsymbol{T}_N, \boldsymbol{p}_1, \dots, \boldsymbol{p}_M]^\top
$$
此时，目标函数写作：
$$
\frac{1}{2} \|\boldsymbol{e}_\mathrm{all}(\boldsymbol{x} + \Delta \boldsymbol{x})\|^2_2 \approx \frac{1}{2} \sum^N_{k = 1} \sum^M_{j = 1} \| \boldsymbol{e}_{k, j} + \boldsymbol{F}^\top_{k, j}\Delta \boldsymbol{T}_k + \boldsymbol{E}^\top_{k, j}\Delta \boldsymbol{p}_{j} \|^2_2
$$
其中：
$$
\boldsymbol{F}^\top_{k, j} = \left. \frac{\partial \boldsymbol{e}(\boldsymbol{T}, \boldsymbol{p}_j)}{\partial \boldsymbol{T}^\top} \right|_{\boldsymbol{T} = \boldsymbol{T}_k}, \boldsymbol{E}^\top_{k, j} = \left. \frac{\partial \boldsymbol{e}(\boldsymbol{T}_k, \boldsymbol{p})}{\partial \boldsymbol{p}^\top} \right|_{\boldsymbol{p} = \boldsymbol{p}_j}
$$
**注意：**不同于第6讲中和原书中的写法，这里的$\boldsymbol{e}$是一个二值多变量的函数。$\boldsymbol{e}_\mathrm{all}(\boldsymbol{x} + \Delta \boldsymbol{x})$指的是在给定状态量$\boldsymbol{x} + \Delta \boldsymbol{x}$时，所有观测数据（像素坐标）误差之和，其结果是个二维向量，两个分量分别对应两个像素坐标，而最终的目标函数是$\boldsymbol{e}_\mathrm{all}$的二范数（欧式距离）。并且这里$\boldsymbol{F}^\top_{k, j}$实际上是对李代数$\boldsymbol{\xi}_k$的求导结果，只不过在问题中李群和李代数可以一一对应，不用过于纠结数学上的表示方法。

现在，把相机位姿变量放在一起（这里把李群改为了李代数表示）：
$$
\boldsymbol{x}_c = [\boldsymbol{\xi}_1,\boldsymbol{\xi}_2,\dots, \boldsymbol{\xi}_N]^\top \in \mathbb{R}^{6N}
$$
并把空间点也放在一起：
$$
\boldsymbol{x}_p = [\boldsymbol{p}_1,\boldsymbol{p}_2,\dots, \boldsymbol{p}_N]^\top \in \mathbb{R}^{3M}
$$
最终，将求和变为矩阵的形式后，上式可简化如下：
$$
\frac{1}{2} \|\boldsymbol{e}_\mathrm{all}(\boldsymbol{x} + \Delta \boldsymbol{x})\|^2_2 =  \| \boldsymbol{e} + \boldsymbol{F}^\top \Delta \boldsymbol{x}_c + \boldsymbol{E}^\top \Delta \boldsymbol{x}_p \|^2_2
$$
这里的雅可比矩阵$\boldsymbol{E}^\top, \boldsymbol{F}^\top$是整体目标函数对整体变量的导数，它将是一个很大的矩阵，里面的每一个小分块需要由$\boldsymbol{E}^\top_{k, j}, \boldsymbol{F}^\top_{k, j}$”拼凑“起来，无论用什么非线性优化方法，最终都会面对增量线性方程：
$$
\boldsymbol{H}\Delta \boldsymbol{x} = \boldsymbol{g}
$$
由于把变量分为了位姿和空间点两类，整体的雅可比矩阵可以分块为：
$$
\boldsymbol{J}^\top = [\boldsymbol{F}^\top \boldsymbol{E}^\top]
$$
以高斯牛顿法为例，有：
$$
\boldsymbol{H} = \boldsymbol{J}\boldsymbol{J}^\top = 
\begin{bmatrix}
\boldsymbol{F}\boldsymbol{F}^\top & \boldsymbol{F}\boldsymbol{E}^\top \\ 
\boldsymbol{E}\boldsymbol{F}^\top & \boldsymbol{E}\boldsymbol{E}^\top 
\end{bmatrix}
$$
**注意：**这里为了和之前的笔记统一，转置符号刚好是和原书中相反的。原书中第6讲写$\boldsymbol{H} = \boldsymbol{J}\boldsymbol{J}^\top$，第9讲写$\boldsymbol{H} = \boldsymbol{J}^\top\boldsymbol{J}$，如果概念不够清楚的话容易混淆。在附录**矩阵求导（1）**中可知，求导的结果有雅可比矩阵和梯度矩阵，它们是转置关系，在这里，把雅可比矩阵记作$\boldsymbol{J}^\top$，把梯度矩阵记作$\boldsymbol{J}$。个人在这里保留了前一种写法，与在程序中如何实现无关）。

由于考虑了所有变量，这个线性方程的维度会非常大，如果直接求逆来计算增量方程，计算量是非常大的。但是，在视觉SLAM中，这个$\boldsymbol{H}$矩阵是有特殊结构的，利用这个特殊结构，可以加速求解过程。

### 9.2.3 稀疏性和边缘化

21世纪视觉SLAM的一个重要进展是认识到了矩阵$\boldsymbol{H}$的系数结构，该结构可以自然、显式地用图优化来表示。

**注意：**矩阵的**稀疏性**是指矩阵中**绝大多数元素为零**的特性，在求解大型线性方程组、图论分析和机器学习领域有重要意义。

具体的细节分析可以省略介绍。总的来说，是因为雅可比矩阵$\boldsymbol{J}^\top_{k, j}$具有稀疏性（只与$k$时刻位姿和路标$j$有关，其他项都是零），其对$\boldsymbol{H}$的贡献也具有稀疏形式。一般情况下，$\boldsymbol{H}$可以看作是一个邻接矩阵，在非对角区域中，不为零的子块反映了在某些位姿观测到某些坐标点的情况，其形状像一个镐子。

对于具有这种稀疏结构的$\boldsymbol{H}$，有很多加速计算的方法。在视觉SLAM中，有一种最常用的手段：Schur消元。在SLAM研究中，也称为**边缘化**（Marginalization）。

边缘化的整体思想就是对$\boldsymbol{H}$先进行消元，求解出$\Delta \boldsymbol{x}_c$，然后把其代入原方程，求解出$\Delta \boldsymbol{x}_p$，具体的实现细节这里省略。从概率的角度来说，实际上是把求$(\Delta \boldsymbol{x}_c, \Delta \boldsymbol{x}_p)$的问题，转化成了先固定$\Delta \boldsymbol{x}_p$，求出$\Delta \boldsymbol{x}_c$，再求$\Delta \boldsymbol{x}_p$的过程。当然，反过来先求$\Delta \boldsymbol{x}_p$，再求$\Delta \boldsymbol{x}_c$也是很常见的。

Schur消元只是实现边缘化的一种方式，还有Cholesky分解等其他方法。

### 9.2.4 鲁棒核函数

由于目标函数是误差项的二范数，当出现误匹配的情况时（这在视觉SLAM中很常见），目标函数会变得非常大。因此，算法会试图优先调整一个错误的值，在这种情况下，即使目标函数下降了很多，但真实数据的误差反而会上升。

出现这种问题的原因是，当误差很大时，二范数增长的很快，使得误匹配数据误差的二范数远远盖过了其他本应该被下降的误差，导致算法不管这些本应该被下降的误差，错误地调整误匹配误差。

解决这种问题的方法是：引入**核函数**，使得误差的二范数度量增长没有那么快，同时保证其光滑性质（否则无法求导）。使得整个优化结果更为稳健，因此又叫作**鲁棒核函数**。

鲁棒核函数有很多种，例如最常用的Huber核：
$$
H(\boldsymbol{e}) = \left \{ 
\begin{matrix} 
\begin{array}{ll} 
\frac{1}{2} \|\boldsymbol{e}\|_2^2 & \|\boldsymbol{e}\|_2 \leq \delta \\ 
\delta \left(\|\boldsymbol{e}\|_2 - \frac{1}{2}\delta \right)  & 其他
\end{array} 
\end{matrix} \right.
$$
当误差的二范数大于某个阈值$\delta$后，函数增长由二次形式变成了一次形式，相当于限制了梯度的最大值。当误差较大时，Huber核函数增长明显低于二次函数，可以缓解误匹配项误差盖过其他误差的情况。

除了Huber核，还有Cauchy核、Tukey核等等，这在g2o、Ceres优化库中都有提供。

### 小结

本节中，重点介绍了BA中的稀疏性问题。在实践中，多数软件库已经实现了相关的细节操作。在实际中，我们需要做的主要是构造BA问题，设置Schur消元，然后调用求解器（Solver）对变量进行优化。

下面，使用Ceres和g2o两个库来做BA。为了体现它们的区别，使用公开数据集BAL，并使用同一套读写代码。

## 9.3 实践：Ceres BA

### 9.3.1 BAL数据集

### 9.3.2 Ceres BA的书写

## 9.4 实践：g2o BA

具体见代码。

## 9.5 小结

本讲描述了这么几件事情：

1. 理解后端的概念：由于前端里程计给出的轨迹和地图是很局部的，在长时间内会有误差累积的问题。因此，在后端优化中，要考虑长时间内的所有数据，在更大的规模上对状态量进行优化，考虑到精度与性能的平衡，实际有许多不同的做法。
2. 将最大后验问题通过贝叶斯公式转化成最大似然和先验乘积的问题，后续的处理有2类：滤波器和非线性优化。
3. 以EKF为代表的滤波器后端的工作原理及优缺点。
4. 非线性优化后端的原理，如何从投影模型到构建最小二乘，以及利用稀疏性来求解增量方程，利用核函数增强鲁棒性。
5. 如何使用g2o和Ceres实际操作后端优化。
