# 第12讲 建图

## 12.1 概述

建图是SLAM（同时定位与建图）中的两大目标之一。在SLAM模型中，地图即所有路标点的集合，确定了路标点的位置，就可以说完成了建图。在前面的章节中，无论是视觉里程计，还是后端，都建模了路标点（例如ORB中的特征点）的位置，并对它们进行了优化。然而，尽管上层应用（例如导航、交互等）对于“定位”的需求是相似的，但是对于“地图”，则需要不同的地图形式和建图算法，具体可归纳如下：

1. **定位**。定位是地图的一项基本功能。例如，在视觉里程计中提到的PnP就利用了局部地图来估计相机位姿。在回环检测中，也提到了可以利用一条已知的轨迹和已建立的地图进行重定位。我们还希望能够把地图保存下来，让机器人在下次开机后可以直接进行定位，而不必每次开机都重新对地图进行建模。
2. **导航**。导航是指机器人能够在地图中进行路径规划，在任意两个地图点间寻找路径，然后控制自己运动到目标点的过程。在该过程中，至少要知道地图中哪些地方可以通过，哪些地方不可以通过。这就超出了稀疏特征点地图的能力范围，必须有另外的地图形式，并且这至少得是一种稠密地图。
3. **避障**。与导航中判断是否可通过的情况类似，但是更注重局部的、动态的障碍物的处理。同样，仅有特征点地图无法判断某个特征点是否为障碍物，也需要稠密地图。
4. **重建**。利用SLAM也可以获得周围环境的重建效果。这种地图也是稠密的，主要用于向人展示，要求美观。并且，不满足于一般的稠密点云重建，还希望构建带纹理的平面（例如游戏中的三维场景）。
5. **交互。**例如AR，可以在场景中使用虚拟物体与地图互动。例如，投掷虚拟物体与墙体发生碰撞。此外，也可以指挥机器人去拿桌子上的报纸，这就需要构建更高级的语义地图了。

在之前章节中的讨论，基本集中于“稀疏路标地图”部分，还没有探讨过稠密地图。所谓稠密地图，是相对于稀疏地图而言的。稀疏地图只建模感兴趣的部分，即之前一直说的特征点（路标点）。而稠密地图是指建模所有看到过的地方。例如，对于一张桌子，稀疏地图可能只建模了桌子的四个角，而稠密地图会建模整个桌面。从“定位”的角度来看，只有四个角的地图也可以对相机位姿进行估计，但是无法推测这四个角点之间的空间结构，因此无法用其完成导航、避障等需要稠密地图才能完成的工作。

现在，根据上面的讨论，已经知道稠密地图的重要性了。下一步，学习如何通过视觉SLAM建立稠密地图。

## 12.2 单目稠密重建

### 12.2.1 立体视觉

在视觉SLAM的稠密重建中，我们需要知道每一个像素点（或大部分像素点）的距离，对此，大致上有以下解决方案：

1. 使用单目相机，估计相机运动，并且利用三角化计算像素的距离。
2. 使用双目相机，利用左右目的视差计算像素的距离。
3. 使用RGB-D相机，直接获得像素距离。

前两种方式称为立体视觉（Stereo Vision），其中，移动单目相机的又称为移动视角的立体视觉（Moving View Stereo，MVS）。相比于RGB-D直接测量的深度，使用单目和双目的方式对深度获取往往是“费力不讨好”的，计算量巨大，最后得到一些不怎么可靠的深度估计（尤其是单目）。因此，使用RGB-D进行稠密重建往往是常见的选择。然而，由于量程等限制，RGB-D还无法被很好地应用在室外、大场景场合中，仍需要通过立体视觉来估计深度信息。

下面，举一个单目稠密重建的例子。

假定有一段视频序列以及每一帧对应的轨迹。现在，以第一幅图像为参考帧，要计算参考帧中每个像素的深度。

首先，回忆在第7讲中对特征点使用三角测量进行深度估计的步骤：

1. 对每个图像提取特征点，并计算特征点之间的匹配。换言之，是对某一个空间点进行了跟踪，知道它在各个图像上的位置。

2. 利用三角测量原理，通过不同视角下的观测，估计某一个特征点的深度。

在稠密深度图估计中，我们无法把每个像素都当作特征点，通过计算描述子来进行匹配，因此要用下一小节讲的**极线搜索**和**块匹配技术**。当知道了某个像素在各个图中的位置，就能利用三角测量确定它的深度，但与第7讲中计算特征点深度不同的是，在这里要使用很多次三角测量让深度估计收敛，而不是仅使用一次。我们希望深度估计能够随着测量的增加从一个非常不确定的量，逐渐收敛到一个稳定值。这就是**深度滤波器技术**。下面的内容也围绕这个主题展开。

### 12.2.2 极线搜索与块匹配

如图所示，左边相机（$\boldsymbol{T}_1$）观测到了某个像素$\boldsymbol{p}_1$，由于这是单目相机，不知道它对应的空间点$\boldsymbol{P}$的实际深度，因此不妨假设深度的范围是$(d_{\min}, +\infty)$，即$\boldsymbol{P}$在空间中的一条射线上。当相机移动到右边（$\boldsymbol{T}_2$）时，这条射线的投影形成了图像平面的一条线段，称为**极线**。当知道相机运动时，这条极线是能够确定的。那么问题就是：极线上的哪一个点才是$\boldsymbol{P}$在$\boldsymbol{T}_2$处的投影$\boldsymbol{p}_2$呢？

在特征点方法中，$\boldsymbol{p}_1$和$\boldsymbol{p}_2$可以用计算描述子来匹配。但是在这里，我们并没有描述子，所以只能在极线上搜索和$\boldsymbol{p}_1$相似的点，具体来说，就是沿着极线从一端走到另一端，逐像素比较与$\boldsymbol{p}_1$的相似度，选最相似的那个点作为$\boldsymbol{p}_2$。然而，单个像素的亮度没有区分性，所以可以取$\boldsymbol{p}_1$周围的一个大小为$w \times w$的小块，然后在极线上也取很多同样大小的块进行比较，可以在一定程度上提高区分性，这就是所谓的**块匹配**。和视觉里程计中的直接法类似，块匹配的方法是基于**图像块的灰度不变性假设**的。

不妨把$\boldsymbol{p}_1$周围的小块记为$\boldsymbol{A} \in \mathbb{R}^{w \times w}$，把极线上的$n$个小块记为$\boldsymbol{B}_i \in \mathbb{R}^{w \times w}, i = 1, \dots, n$。比较小块之间的**相似度**有若干种方法：

1. SAD（Sum of Absolute Difference）
   $$
   \mathrm{SAD}(\boldsymbol{A}, \boldsymbol{B}) = \sum_{i, j} |\boldsymbol{A}(i, j) - \boldsymbol{B}(i, j)|
   $$

2. SSD（Sum of Squared Distance）
   $$
   \mathrm{SSD}(\boldsymbol{A}, \boldsymbol{B}) = \sum_{i, j} (\boldsymbol{A}(i, j) - \boldsymbol{B}(i, j))^2
   $$

3. NCC（Normalized Cross Correlation）
   $$
   \mathrm{NCC}(\boldsymbol{A}, \boldsymbol{B}) = \frac{\sum_{i, j} \boldsymbol{A}(i, j)\boldsymbol{B}(i, j)}{\sqrt{\sum_{i, j} \boldsymbol{A}^2(i, j) \sum_{i, j}  \boldsymbol{B}^2(i, j)}}
   $$

这些计算方式往往存在精度和效率之间的矛盾，需要在实际工程中进行折衷。此外，除了这些简单版本，也可以先**把每个小块的均值去掉**，也就是说，允许像“$\boldsymbol{B}$小块比$\boldsymbol{A}$小块整体上里亮一些，但仍然很相似”这样的情况，这往往也更加可靠。此外，还有更多的块匹配度量方法，可以借助其他材料补充学习。

学会了极线搜索与块匹配，就可以沿着极线，使用某个块匹配度量（假设使用NCC）计算$\boldsymbol{A}$和所有$\boldsymbol{B}_i$的相似度，得到一个沿着极线的NCC分布。这个分布的形状取决于图像数据，并且在搜索距离较长的情况下，通常会得到一个非凸函数：存在许多峰值，但是真实的对应点必定只有一个。在这种情况下，我们倾向于使用概率分布描述深度值。于是，我们的问题就变为：在不断对不同图像进行极线搜索时，我们估计的深度分布将发生怎样的变化——这就是所谓的**深度滤波器**。

### 12.2.3 高斯分布的深度滤波器

对像素点深度的估计也可建模为一个状态估计问题，对应的，就有滤波器法和非线性优化两种求解思路。在SLAM这种实时性要求较强的场合，考虑到前端已经占据了不少的计算量，建图方面通常采用计算量较少的滤波器方式。

对深度的分布假设存在若干种做法，考虑简单的情况，可以假设某个像素点的深度$d$服从高斯分布：
$$
P(d) = N(\mu, \sigma^2)
$$
每当新的数据到来，我们都会观测到它的深度。同样，假设这次观测也是一个高斯分布：
$$
P(d_{\mathrm{obs}}) = N(\mu_{\mathrm{obs}}, \sigma_{\mathrm{obs}}^2)
$$
用新的观测信息$d_{\mathrm{obs}}$更新原先$d$的分布（信息融合），根据高斯分布的乘积，有：
$$
\mu_{\mathrm{fuse}} = \frac{\sigma_{\mathrm{obs}}^2\mu + \sigma^2\mu_{\mathrm{obs}}}{\sigma^2 + \sigma_{\mathrm{obs}}^2}, \quad \sigma_{\mathrm{fuse}}^2 =  \frac{\sigma^2\sigma_{\mathrm{obs}}^2}{\sigma^2 + \sigma_{\mathrm{obs}}^2}
$$
上面信息融合的步骤就类似于卡尔曼滤波器的“更新”，由于没有运动方程，也就没有了“预测”部分（也可以看成是固定深度值不动）。接下来的问题是：如何计算$\mu_{\mathrm{obs}}, \sigma_{\mathrm{obs}}^2$？

**注：**在有的更复杂的方法中，假设了深度服从均匀-高斯混合分布。并且，在上面的推导中，不明白为什么是**乘积**。也没理解高斯分布乘积后的均值和方差为什么是这个（好像和$E(AB) = E(A)E(B)$不一样?）。暂且先这么记着。

对于计算$\mu_{\mathrm{obs}}, \sigma_{\mathrm{obs}}^2$，也有不同的处理方式，可以考虑几何不确定性和光度不确定性之和，也可以简单地只考虑由几何关系带来的不确定性。假设我们只考虑几何不确定性，并通过极线搜索和块匹配确定了参考帧的某个像素在当前帧的投影位置，要如何估计这个位置对深度的不确定性有多大？

考虑某次极线搜索，找到了$\boldsymbol{p}_1$对应的$\boldsymbol{p}_2$，从而观测到了（这里的“观测到了”实际上指的是三角计算）$\boldsymbol{p}_1$的深度，即找到了空间点$\boldsymbol{P}$。从而，可将$\boldsymbol{O}_1\boldsymbol{P}$记为$\boldsymbol{p}$，$\boldsymbol{O}_1\boldsymbol{O}_2$记为$\boldsymbol{t}$，$\boldsymbol{O}_2P$记为$\boldsymbol{a}$。并且，把这个三角形下面的两个角记作$\alpha, \beta$。现在，考虑极线$l_2$上存在一个像素大小的误差，使得$\beta$变为了$\beta'$，$\boldsymbol{p}_2$变为了$\boldsymbol{p}_2'$，三角形上面的角变为了$\gamma$，而$\boldsymbol{p}$也变为了$\boldsymbol{p}'$。现在的问题是，$\boldsymbol{p}$与$\boldsymbol{p}'$之间的误差是多少？

列写几何关系，有：
$$
\boldsymbol{a} = \boldsymbol{p} - \boldsymbol{t} \\
\alpha = \arccos \langle \boldsymbol{p}, \boldsymbol{t} \rangle \\ 
\beta = \arccos \langle \boldsymbol{a}, -\boldsymbol{t} \rangle
$$
对$\beta$进行扰动一个像素后得到$\beta'$，有：
$$
\beta' = \arccos \langle \boldsymbol{O}_2\boldsymbol{p}'_2, -\boldsymbol{t} \rangle \\
\gamma = \pi - \alpha - \beta'
$$
由正弦定理，得：
$$
\| \boldsymbol{p}' \|_2 = \| \boldsymbol{t} \|_2 \frac{\sin \beta}{\sin \gamma}
$$
由此，如果认为极线搜索的块匹配仅有一个像素的误差，那么就可以设：
$$
\sigma_{\mathrm{obs}} = \| \boldsymbol{p} \|_2 - \| \boldsymbol{p}' \|_2
$$
如果极线搜索的不确定性大于一个像素，则我们可按照此推导放大这个不确定性，接下来，就是前面提到的信息融合了，然后依次迭代。在实际工程中，当不确定性小于一定阈值时，就可以认为深度数据已经收敛了。

**注：**这里好像没有提到均值$\mu_{\mathrm{obs}}$，个人理解可以设置为$\| \boldsymbol{p} \|_2$。

综上，估计稠密深度的完成过程可归纳如下：

1. 假设所有的像素深度满足某个初始的高斯分布（初始化）。
2. 当新的数据产生时，通过极线搜索和块匹配确定投影点位置。
3. 根据几何关系，计算三角化后的深度的均值和不确定性。
4. 将当前的观测结果融合到上一次的估计中。若收敛则停止计算，否则返回第2步。

这些步骤组成了一套可行的深度估计方式。并且需要注意的是，这里说的深度是指$\boldsymbol{O}_1\boldsymbol{P}$的长度，而不是相机坐标系下的$Z$值。

## 12.3 实践：单目稠密重建

具体细节见代码。步骤为：遍历每个像素$\boldsymbol{p}_1$ -> 利用当前的深度（即均值）和方差，找到$\boldsymbol{p}_2$和极线的长度和方向 -> 以$\boldsymbol{p}_2$为中心进行极线搜索，计算NCC，找到最好的匹配，更新$\boldsymbol{p}_2$ -> 利用新的$\boldsymbol{p}_1, \boldsymbol{p}_2$进行三角测量，得到深度（即均值）和方差，并进行信息融合 -> 迭代。

### 12.3.1 实验分析与结论

在实践过程中，通过移动单目相机的稠密建图，用滤波器的方式估计了参考帧的每个像素深度。代码相对直接，没有使用许多的技巧（trick），因此，出现了实际工程中常见的情形——简单的往往并不是最有效的。

由于真实数据的复杂性，能够在实际环境中工作的程序往往需要大量的工程技巧，这使得每种实际可行的代码都极其复杂。但是为了方便，示例中采用了易读易写的实现方式。

### 12.3.2 像素梯度的问题

显然，块匹配的正确与否依赖于图像块是否具有区分度。如果图像块仅是一片白或一片黑，缺少有效的信息，那么在NCC计算中就很可能错误地将它与周围的某块像素匹配。例如实践中所用图片上的打印机。

显然，有明显梯度的小块将具有良好的区分度，不易引起误匹配。对于梯度不明显的像素，由于在块匹配时没有区分性，将难以有效地估计其深度。反之，像素梯度比较明显的地方，我们得到的深度信息也相对准确。因此，实践程序也反映了立体视觉中一个非常常见的问题：**对物体纹理的依赖性**。

在本节的实践程序中，刻意使用了纹理较好的环境，例如棋盘格地板，带木纹的桌面等，因此效果看似不错。然而，在实际的环境中，纯色墙面、地板等表面亮度均匀的地方将经常出现，影响深度估计。如果我们依然只关心某个像素周围的邻域（小块）的话，无法在现有的算法流程上解决该问题。

进一步讨论像素梯度问题，还会发现像素梯度和极线之间的联系。

举两种比较极端的情况：像素梯度平行于极线方向和垂直于极线方向。在平行的例子中，可以很容易精确地确定匹配度最高点出现在何处。反之。在垂直的例子中，即使小块有明显梯度，当沿着极线做块匹配时，NCC的计算结果也都是一样的，得不到有效的匹配。实际的情况往往介于二者之间。当像素梯度与极线夹角较大时，极线匹配的不确定度大，当夹角较小时，匹配的不确定度则变小。在演示程序中计算不确定度时，我们把这些情况的像素误差都设置成了1，实际是不够精细的。

### 12.3.3 逆深度

首先是讨论一下参数化的问题，在前面的内容中，通常用$x, y, z$来描述空间点，可以认为这3个量都是随机的，服从三维高斯分布，这是一种参数化形式。本讲（稠密建图）则使用了像素坐标$u, v$和深度$d$来描述某个空间点。认为$u, v$不动，而$d$服从一维高斯分布，这是另一种参数化形式。

不同的参数化形式，实际上都描述了同一个事实，即一个空间点的坐标。考虑当在相机中看到某个点时，它的图像坐标$u, v$是比较确定的，而深度值$d$是非常不确定的。此时，若用世界坐标$x, y, z$来描述这个点，根据相机当前的位姿，这3个量之间可能存在明显的相关性。而如果用$u, v, d$来描述，那么它的$u, v$和$d$则是近似独立的，从而获得更简洁的协方差矩阵。

逆深度（Inverse depth）是近年来SLAM研究中出现的一种广泛使用的参数化技巧。在演示程序中，我们假设了$d \sim N(\mu, \sigma^2)$，然而，这是有一些问题的：

1. 这个分布并不能像高斯分布那样有一个完全对称的形状，至少负数区域是为零的。
2. 在室外，图片中可能存在距离非常远，乃至无限远处的点（例如云朵、天空），我们不可能初始化那么大的深度，并且用高斯分布描述它们会出现一些数值计算上的困难。

于是，逆深度应运而生。人们在仿真中发现，假设深度$d$的倒数$d^{-1}$，也就是逆深度，为高斯分布是比较有效的。随后，在实际应用中，逆深度也具有更好的数值稳定性，逐渐成为一种通用的技巧，存在于现有SLAM方案中的标准做法。

### 12.3.4 图像间的变换

在做块匹配之前，做一次图像到图像间的变换是一种常见的预处理方式。这是因为，我们假设了图像小块在相机运动时保持不变，这在相机平移或者是做微小旋转时（实践所用的数据集就是这样的）是成立的，但如果发生了明显旋转，显然是不行的。

为了防止这种情况的出现，在块匹配之前，通常需要考虑参考帧与当前帧之间的运动。根据相机模型，参考帧上的一个像素$\boldsymbol{Px}_{\mathrm{ref}}$与真实的三维点世界坐标$\boldsymbol{P}_{\mathrm{world}}$有以下关系：
$$
d_{\mathrm{ref}}\boldsymbol{Px}_\mathrm{ref} = \boldsymbol{K}(\boldsymbol{R}_{\mathrm{world2ref}}\boldsymbol{P}_{\mathrm{world}} + \boldsymbol{t}_{\mathrm{world2ref}})
$$
类似地，对于当前帧，也有$\boldsymbol{P}_{\mathrm{world}}$在它上面的投影$\boldsymbol{Px}_{\mathrm{camera}}$：
$$
d_{\mathrm{camera}}\boldsymbol{Px}_\mathrm{camera} = \boldsymbol{K}(\boldsymbol{R}_{\mathrm{world2camera}}\boldsymbol{P}_{\mathrm{world}} + \boldsymbol{t}_{\mathrm{world2camera}})
$$
联立两式，消去$\boldsymbol{P}_{\mathrm{world}}$，有：
$$
d_{\mathrm{camera}}\boldsymbol{Px}_\mathrm{camera} = d_{\mathrm{ref}}\boldsymbol{K}\boldsymbol{R}_{\mathrm{world2camera}}\boldsymbol{R}_{\mathrm{world2ref}}^\top\boldsymbol{K}^{-1}\boldsymbol{Px}_\mathrm{ref} + \boldsymbol{K}\boldsymbol{t}_{\mathrm{world2camera}} - \boldsymbol{K}\boldsymbol{R}_{\mathrm{world2camera}}\boldsymbol{R}_{\mathrm{world2ref}}^\top\boldsymbol{t}_{\mathrm{world2ref}}
$$
当知道$d_{\mathrm{ref}},\boldsymbol{Px}_\mathrm{ref}$时，可以计算出$\boldsymbol{Px}_\mathrm{camera}$（第三维归一化）。此时，再给$\boldsymbol{Px}_\mathrm{ref}$两个分量各一个增量$\mathrm{d}u, \mathrm{d}v$，就可以求得$\boldsymbol{Px}_\mathrm{camera}$的增量$\mathrm{d}u_\mathrm{c}, \mathrm{d}v_\mathrm{c}$。通过这种方式，算出在局部范围内参考帧和当前帧图像坐标变换的一个线性关系，构成仿射变换：
$$
\begin{bmatrix}
\mathrm{d}u_\mathrm{c} \\ 
\mathrm{d}v_\mathrm{c}
\end{bmatrix}

= 

\begin{bmatrix}
\frac{\mathrm{d}u_\mathrm{c}}{\mathrm{d}u} & \frac{\mathrm{d}u_\mathrm{c}}{\mathrm{d}v}\\ 
\frac{\mathrm{d}v_\mathrm{c}}{\mathrm{d}u} & \frac{\mathrm{d}v_\mathrm{c}}{\mathrm{d}v}
\end{bmatrix}

\begin{bmatrix}
\mathrm{d}u \\ 
\mathrm{d}v
\end{bmatrix}
$$
根据仿射变换矩阵，我们可以将当前帧的像素进行变换，再与参考帧进行块匹配，可以获得对旋转更好的效果。

**注：**这里的深度貌似是空间坐标的第三维，即相机坐标系下的$Z$值，而不是指$\boldsymbol{O}_1\boldsymbol{P}$的长度。并且这里的仿射变换写的有点突兀，起码系数不对，不知道实际程序中是怎么写的。

### 12.3.5 并行化：效率的问题

在实验中，我们也看到，稠密深度图的估计非常费时，即使现在主流的CPU也无法实时地计算那么多的像素。不过，该问题也存在另一个性质：这些像素的深度估计是彼此无关的。因此，可以利用**GPU**进行**并行计算**，用多个线程同时对每个像素进行深度估计，然后将结果统一起来，而不必像例程那样写一个双重循环。

### 12.3.6 其他的改进

事实上，还有许多对本例程进行改进的方案，例如：

1. 在例程中，各像素的深度完全是独立计算的，在实际计算的过程中，可能存在对某个像素的深度估计很小，边上一个又很大的情况（就像那个打字机）。我们可以假设深度图中相邻的深度变化不会太大，给深度估计加上一个空间正则项（可以看作是一种约束）。这种做法会使得到的深度图更加平滑。
2. 由于遮挡、光照、运动模糊等各种因素的影响，数据中可能会有外点（Outlier），进而可能会出现误匹配的情况。举个例子，假如镜头上有一个污点，在参考帧中，污点遮挡光线，使图片最左上角的一个小块变黑了，在当前帧中，它仍然使图片最左上角这个小块变黑，那么在匹配的过程中，由于NCC大于阈值，两个相同位置的黑块完成了匹配（这种情况下，算到的深度值应该是无穷大的），但其实这个块是随相机发生了运动的。
3. 处理错误匹配的方式有很多种。例如，有文献提出均匀-高斯混合分布下的深度滤波器，显式地将内点与外点进行区别并进行概率建模，能够较好地处理外点数据。

总而言之，存在许多可能的改进方案。如果细致地改进每一步的做法，最后是有希望得到一个良好的稠密建图方案的。然而，从理论上来看，有些问题很难通过调整代码来解决，例如对环境纹理的依赖、像素梯度和极线方向的关联等。所以，到目前为止，虽然立体视觉能够建立稠密地图，但是我们通常认为其过于依赖环境纹理和光照，不够可靠。

## 12.4 RGB-D稠密建图

除了立体视觉，在适用范围内，RGB-D相机是一种更好的选择，可以直接通过传感器硬件测量深度，无须消耗大量的计算资源，并且不依赖环境纹理。

利用RGB-D进行稠密建图是相对容易的。根据地图形式的不同，也存在若干种不同的主流建图方式。其中，最直观、最简单的方法就是根据估算的相机位姿，将RGB-D数据转化为点云，然后进行拼接，最后得到一个由离散的点组成的点云地图（Point Cloud Map）。在此基础上，如果我们对外观有进一步的要求，希望估计物体的表面，则可以使用三角网格（Mesh）、面片（Surfel）进行建图。另外，如果希望知道地图的障碍物信息并在地图上导航，也可通过体素（Voxel）建立占据网格地图（Occupancy Map）。

**注：**体素是一种三维的像素。

RGB-D建图涉及的理论知识并不是很多，在下面几节，直接通过实践来介绍。

### 12.4.1 实践：点云地图

所谓点云，就是由一组离散的点表示的地图。最基本的点包含$x, y, z$三维坐标，也可以带有$r, g, b$的彩色信息。RGB-D相机提供了彩色图和深度图，因此很容易根据相机内参来计算RGB-D点云。如果通过某种手段得到了相机的位姿，那么只要直接把点云进行加和，就可以获得全局的点云。

在实际建图中，我们还会对点云加一些滤波处理，以获得更好的视觉效果。在本程序中，主要使用两种滤波器：外点去除滤波器和体素网格的降采样滤波器（Voxel grid filter）。

具体的细节见代码。

几个要注意的点：

1. 在生成每帧点云时，去掉深度值无效的点。
2. 利用统计滤波器方法去除孤立点。统计每个点与距离它最近的$N$个点的距离值的分布，去除距离均值过大的点。
3. 利用体素网格滤波器进行降采样，去掉重叠点，节省大量存储空间。

点云可以提供比较基本的可视化地图，让我们能大致了解环境的样子。点云的一大优势是可以直接由RGB-D图像高效地生成，不需要额外处理，滤波操作也比较直观。但是，点云地图更接近传感器读取的原始数据，只具有一些基本的功能，而不能直接用于应用程序。使用点云表达地图是十分初级的，不过，如果希望地图有更高级的功能，点云地图是一个不错的出发点，大部分由点云转换得到的地图形式都在PCL库中提供。

### 12.4.2 从点云重建网格

在刚刚的点云文件的基础上，可以很容易地建立网格。大致思路是先计算点云的法线，再从法线计算网格。

具体见代码。

### 12.4.3 八叉树地图

在介绍八叉树地图前，先提一下点云地图的两个明显缺陷：

1. 点云地图的规模很大，在之前的实验中，一幅分辨率为640 x 480的图像产生了30万个空间点，即使经过了一些滤波，pcd文件也很大，需要大量存储空间。并且，其中的很多细节是不必要的，例如毛毯上的褶皱、阴暗处的影子等。除非降低分辨率（然而，这会降低地图质量），否则无法在有限的内存中建模较大的环境。因此，需要用合理的方式对地图进行压缩存储，舍弃无用信息。
2. 点云地图无法处理运动物体。在实验中，我们只执行了拼接点云的“加”操作，而没有“当点消失时把它移除”的操作。然而，在实际环境中，运动的物体普遍存在，例如人，这是不需要建模到地图里的物体。因此，这样的点云地图是不实用的。

下面要介绍的八叉树地图（Octomap）是一种灵活的、有较好的压缩性能的、能随时更新的地图，在导航中比较常用。

把三维空间建模为许多个小方块（或体素）是一种常见的做法。易知，一个方块可以分成同样大小的八个更小的方块，并可以不断细分，直到最后的方块大小达到建模的最高精度。在这个过程中，把“将一个方块平均分成八个小方块”可以看成“从一个节点展开成八个子节点”，那么，整个从最大空间细分到最小空间的过程，就是一棵八叉树。

如果用整个大方块代表整个地图，也就是根节点。那么最小的块就是叶子节点。如果叶子节点的方块大小为$1\mathrm{cm}^3$，限制八叉树为10层时，总共能建模的体积的大小约为$8^{10}\mathrm{cm}^3 \approx 1073 \mathrm{m}^3$，这足够建模一间屋子了。并且，由于体积与深度呈指数关系，用更大的深度时，可建模的体积会增长得非常快。

在八叉树中，节点中存储的是对应的空间位置是否被占据的信息，用**概率对数**表示。因此，当某个方块的所有子节点都被占据或都不被占据时，就没必要展开这个节点。举一个最极端的情况，整个地图都是空白的，这时只需要一个根节点就可以了。实际中，地图中的物体经常是连在一起的，而空白的地方也会常常连在一起，所以大多数八叉树节点无须展开到叶子层，可以节省大量的存储空间。

若某个节点的占据信息（概率对数）为$y \in \mathbb{R}$，则这个节点被占据的概率$x$为：
$$
x = \mathrm{logit}^{-1}(y) = \frac{\exp(y)}{\exp(y) + 1}
$$
当不断观测到“占据”时，让$y$增加，反之减小。通过这种方式，可以动态地对占据信息进行调整，建模地图中的障碍物信息。

有了对数概率，就可以根据RGB-D数据更新整个八叉树地图了。假设RGB-D图像中观测到某个像素带有深度$d$，就说明：**在深度值对应的空间点上观察到了一个占据数据，并且，从相机光心出发到这个点的线段上应该是没有物体的**（否则会被遮挡）。利用这个信息，可以很好地对八叉树地图进行更新，并且能处理运动的结构。

### 12.4.4 实践：八叉树地图

具体见代码。

## 12.5 *TSDF地图和Fusion系列

略。

## 12.6 小结

本讲介绍了一些常见的地图类型，尤其是稠密地图形式。立体视觉（单目 + 移动，双目）和RGB-D都可以构建稠密地图。相比之下，在适用范围内，RGB-D要更实用。本讲的地图侧重于度量地图，而拓扑地图的形式与SLAM研究差别比较大，因此没有详细探讨。
